{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template for Loading and Training Model\n",
    "\n",
    "This is a template for developing TensorFlow models using several high-level APIs.\n",
    "<ul>\n",
    "    <li><a href='https://www.tensorflow.org/api_docs/python/tf/contrib/training/HParams'>HParams</a></li>\n",
    "    <li><a href='https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig'>RunConfig</a></li>\n",
    "    <li><a href='https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset'>Dataset</a> with an <a href='https://www.tensorflow.org/api_docs/python/tf/contrib/data/Iterator'>Iterator</a></li>\n",
    "    <li><a href='https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator'>Estimators</a> - <a href='https://www.tensorflow.org/extend/estimators'>Creating Estimators</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time, datetime, shutil, os, glob\n",
    "from model import model_fn, params, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "tf.set_random_seed(0) # Graph level random seed\n",
    "np.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gather data file paths\n",
    "train_dirs = glob.glob('../train/*')\n",
    "train_files = []\n",
    "for d in train_dirs:\n",
    "    train_files.extend(glob.glob(d + '/*.tfrecord'))\n",
    "\n",
    "print(str(len(train_files)) + ' train_files.')\n",
    "\n",
    "valid_dirs = glob.glob('../valid/*')\n",
    "valid_files = []\n",
    "for d in valid_dirs:\n",
    "    valid_files.extend(glob.glob(d + '/*.tfrecord'))\n",
    "\n",
    "print(str(len(valid_files)) + ' valid_files.')\n",
    "data_files = {'train' : train_files, 'valid' : valid_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ts = time.time()\n",
    "timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_dir = '/tmp/' + timestamp\n",
    "print('Output directory at:\\n' + model_dir)\n",
    "\n",
    "params = get_hyper_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "def parser(record):\n",
    "    # Define features\n",
    "    features={\n",
    "        'height': tf.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "        'label': tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    # Parse out data\n",
    "    parsed = tf.parse_single_example(record, features)\n",
    "    # Can now get feature with parsed['feature']\n",
    "    \n",
    "    # Data augmentation, etc\n",
    "    \n",
    "    return {'feature' : value }, label\n",
    "\n",
    "def load_dataset(mode):\n",
    "    # Load the dataset\n",
    "    files = data_files[mode]\n",
    "    dataset = tf.contrib.data.TFRecordDataset(files)\n",
    "\n",
    "    # Parse the TFRecord entries\n",
    "    dataset = dataset.map(parser, num_parallel_calls=2)\n",
    "\n",
    "    # Shuffle the data if training\n",
    "    if mode == 'train':\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    # Batch the data\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def dataset_input_fn(mode):\n",
    "    dataset = load_dataset(mode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at some of the data to make sure we are \n",
    "#   saving/reloading correctly\n",
    "% matplotlib inline\n",
    "\n",
    "features, labels = dataset_input_fn('train')\n",
    "print('Features keys: ' + str(features.keys()))\n",
    "print('Labels shape: ' + str(labels.shape) + '\\n')\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "feats, labs = sess.run([features, labels])\n",
    "print('Features: ' + str(features.keys()) + '\\n')\n",
    "print('Labels shape: ' + str(labels.shape))\n",
    "\n",
    "for k in feats.keys():\n",
    "    print(k + ' shape: ' + str(feats[k].shape) + ', dtype: ' + str(feats[k].dtype))\n",
    "\n",
    "image = (feats['start'][0]) / 256.\n",
    "label = labels[0]\n",
    "print('First label: ' + str(label))\n",
    "\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Model\n",
    "tf.reset_default_graph()\n",
    "estimator = tf.estimator.estimator(model, model_dir, config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training/Evaluation Loops\n",
    "print('tensorboard --logdir=' + str(output_dir))\n",
    "for e in range(hyper_params.train_epochs):\n",
    "    print('Epoch: ' + str(e))\n",
    "    estimator.train(input_fn=lambda: dataset_input_fn('train'))\n",
    "    estimator.evaluate(input_fn=lambda: dataset_input_fn('valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: This won't work in a Jupyter Notebook because __file__ is not\n",
    "#       defined, but should work in a pure .py python file. '''\n",
    "\n",
    "# Find the path of the current running file (train script)\n",
    "curr_path = os.path.realpath(__file__)\n",
    "\n",
    "# Define the path of your factored out model.py file\n",
    "model_file = '/some/path/model.py'\n",
    "\n",
    "# Now copy the training script and the model file to \n",
    "#   model_dir -- the same directory specified when creating the Estimator\n",
    "# Note: copy over more files if there are other important dependencies.\n",
    "shutil.copy(curr_path, model_dir)\n",
    "shutil.copy(model_path, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env]",
   "language": "python",
   "name": "Python [env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
